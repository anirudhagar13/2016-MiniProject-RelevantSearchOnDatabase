#include<iostream>
#include<fstream>
#include<sstream>
#include<vector>
#include<string>
#include<map>
#include<algorithm>
using namespace std;

void write_to_file(vector<string>& mp)
{
	ofstream myfile;
	myfile.open ("output-tokenizer.cpp");

  	//Just replace myfile with cout to print on stdout

  	for(auto e : mp)
	{
		myfile<<e<< "\n";
	}

	myfile.close();
}
vector<string>& tokenizer_line(vector<string>& words,string line,string delim) // takes one line as the input every time-tokenizes line by line
{
	size_t prev = 0,pos;
        while((pos = line.find_first_of(delim,prev)) != string::npos)
        {
            if(pos > prev)
                words.push_back(line.substr(prev,pos-prev));
            prev = pos+1;
        }
        if(prev < line.length())
            words.push_back(line.substr(prev,string::npos));

	return words;
	
}
#if 1
void create_index1(map<string,vector<unsigned int>>& mp, const vector<string>& v,unsigned int tid)
{
	for(auto e : v)
	{

		if(mp.find(e) != mp.end())
			mp[e].push_back(tid);
		else
			mp[e] = vector<unsigned int>({tid});
			
	}
}
#endif
#if 1
void display_map(const map<string,vector<unsigned int>>& mp)
{
	for(auto key : mp)
	{
		cout<<key.first<<" : {";
		for(auto vl : key.second)
			cout<<vl<<",";
		cout<<" }\n";
	}
}
#endif
template<typename Iterator>
void display_vector(Iterator& begin, Iterator& end)
{	
	while(begin != end)
	{
		cout << *begin << " "; 
		++begin;
	}
	cout << "\n";
}
void display_vector(const vector<string>& v)
{
	for(auto e : v)
		cout << e << " ";

	cout << "\n";
}
int main()
{
	ifstream input_file("sample_input_file.csv");
	string line;
    	vector<string> col_data; // contains data of cells column-wise
	vector<string> cell_data; // contains data of every cell
	unsigned int tid;
	map<string,vector<unsigned int>> col_index;
	map<string,vector<unsigned int>> cell_index;	
    	while(getline(input_file,line))
    	{
		col_data = tokenizer_line(col_data,line,","); // column index(",")
		//display_vector(col_data.begin(),col_data.end());
		display_vector(col_data);
		//cell_data = tokenizer_line(words,line," ,"); // cell index(" ,")
		create_index1(col_index,col_data,tid); // create_index1() - col_index,cell_index
		//create_index1(cell_index,cell_data);
		//create_index2(); // create_index2() - tuple_index
		tid++;
    	}
	//display_map(col_index);

    	return 0;
}
